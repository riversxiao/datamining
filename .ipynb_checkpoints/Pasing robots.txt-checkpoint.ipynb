{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first,we'll parse the robots.txt to avoid downlaoding blocked URLs\n",
    "\n",
    "urllib is sooo cool , it comes with robotparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib import robotparser\n",
    "\n",
    "rp = robotparser.RobotFileParser()\n",
    "rp.set_url('https://www.example.com/robots.txt')\n",
    "rp.read()\n",
    "url = 'https://www.example.com'\n",
    "user_agent ='BadCrawler'\n",
    "rp.can_fetch(user_agent,url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the above code gives whether a particular webcrawler can access the webpage or not\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_robots_parser(robots_url):\n",
    "    \" Return the robots parser object using the robots_url \"\n",
    "    rp = robotparser.RobotFileParser()\n",
    "    rp.set_url(robots_url)\n",
    "    rp.read()\n",
    "    return rp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we define the get_robots_parser, which return the robot_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-bebbc333756b>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-bebbc333756b>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    ...\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def link_crawler(start_url, link_regex, robots_url=None, user_agent='wswp'):\n",
    "    ...\n",
    "    if not robots_url:\n",
    "        robots_url = '{}/robots.txt'.format(start_url)\n",
    "    rp = get_robots_parser(robots_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download(url,user_agent='naruto',number_retries = 2, charset='utf-8'):\n",
    "    print(\"Downloading:\", url)\n",
    "    request = urllib.request.Request(url)\n",
    "    request.add_header(\"User-agent\", user_agent)\n",
    "    try:\n",
    "        resp = urllib.request.urlopen(request)\n",
    "        #verify if web charset is utf-8, if not ,we'll use utf-8 to decode, haha, \n",
    "        #it will throw error if no cs returned, but\n",
    "        #we hope utf-8 will help\n",
    "        #read more on pypi.python.org/pypi/chardet to implement a more robust decoder\n",
    "        cs = resp.headers.get_content_charset()\n",
    "        if not cs:\n",
    "            cs = charset\n",
    "        html = resp.read().decode(cs)\n",
    "    except (URLError, HTTPError, ContentTooShortError) as e:\n",
    "        print('Download error:', e.reason)\n",
    "        html = None\n",
    "        if number_retries > 0:\n",
    "            if hasattr(e, \"code\") and 500 <= e.code <= 600:\n",
    "                #recursive download process\n",
    "                return download(url,number_retries -1)\n",
    "    return html\n",
    "\n",
    "def get_robots_parser(robots_url):\n",
    "    \" Return the robots parser object using the robots_url \"\n",
    "    rp = robotparser.RobotFileParser()\n",
    "    rp.set_url(robots_url)\n",
    "    rp.read()\n",
    "    return rp\n",
    "\n",
    "def link_crawler(start_url, link_regex, robots_url=None, user_agent='naruto'):\n",
    "    \n",
    "    \"\"\"\n",
    "    we'll start from a base url and if the link match the link_regex,\n",
    "    we'll continue the crawler process\n",
    "    \"\"\"\n",
    "    if not robots_url:\n",
    "        robots_url = '{}/robots.txt'.format(start_url)\n",
    "    rp = get_robots_parser(robots_url)  \n",
    "    \n",
    "    crawl_queue = [start_url]\n",
    "    seen = set(crawl_queue)\n",
    "    while crawl_queue:\n",
    "        url = crawl_queue.pop()\n",
    "        if rp.can_fetch(user_agent,url):\n",
    "            html = download(url,user_agent=user_agent)\n",
    "            if not html:\n",
    "                continue\n",
    "            #the following code is to get the type of links we want\n",
    "            for link in get_links(html):\n",
    "                if re.match(link_regex, link):\n",
    "                    abs_link = urljoin(start_url, link)\n",
    "                    #let's check whether link is in crawl_queue\n",
    "                    if abs_link not in seen:\n",
    "                        seen.add(abs_link)\n",
    "                        crawl_queue.append(abs_link)\n",
    "        else:\n",
    "            print(\"Blocked By robots.txt:\", url)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
