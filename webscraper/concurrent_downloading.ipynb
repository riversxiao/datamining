{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## concurrent downloading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, it takes long time to download the huge website if download them sequentially, however, if we want to minimize the time spent there, concurrent download is the best choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. one million web pages\n",
    "\n",
    "2. sequential crawler\n",
    "\n",
    "3. threaded crawler\n",
    "\n",
    "4. multiprocessing crawler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one million web pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the one million web pages have already been given as a csv file,here we will extract them from the zip file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO, TextIOWrapper\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resp = requests.get('http://s3.amazonaws.com/alexa-static/top-1m.csv.zip', stream=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "urls = [] # top 1 million URL's will be stored in this list \n",
    "with ZipFile(BytesIO(resp.content)) as zf:\n",
    "    csv_filename = zf.namelist()[0]\n",
    "    with zf.open(csv_filename) as csv_file:\n",
    "        for _, website in csv.reader(TextIOWrapper(csv_file)):\n",
    "            urls.append('http://' + website)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AlexaCallback:\n",
    "    def __init__(self, max_urls=500):\n",
    "        self.max_urls = max_urls\n",
    "        self.seed_url = 'http://s3.amazonaws.com/alexa-static/top-1m.csv.zip'\n",
    "        self.urls = []\n",
    "\n",
    "    def __call__(self):\n",
    "        resp = requests.get(self.seed_url, stream=True)\n",
    "        with ZipFile(BytesIO(resp.content)) as zf:\n",
    "            csv_filename = zf.namelist()[0]\n",
    "            with zf.open(csv_filename) as csv_file:\n",
    "                for _, website in csv.reader(TextIOWrapper(csv_file)):\n",
    "                    self.urls.append('http://' + website)\n",
    "                    if len(self.urls) == self.max_urls:\n",
    "                        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threaded crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "explanation : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing a multithreaded crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
