{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I'll combine the previous crawlers and make it into the final version "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib.request\n",
    "from urllib import robotparser\n",
    "from urllib.parse import urljoin\n",
    "from urllib.error import URLError, HTTPError, ContentTooShortError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "import time\n",
    "\n",
    "\n",
    "class Throttle:\n",
    "    \"\"\" Add a delay between downloads to the same domain\n",
    "    \"\"\"\n",
    "    def __init__(self, delay):\n",
    "        # amount of delay between downloads for each domain\n",
    "        self.delay = delay\n",
    "        # timestamp of when a domain was last accessed\n",
    "        self.domains = {}\n",
    "\n",
    "    def wait(self, url):\n",
    "        domain = urlparse(url).netloc\n",
    "        last_accessed = self.domains.get(domain)\n",
    "\n",
    "        if self.delay > 0 and last_accessed is not None:\n",
    "            sleep_secs = self.delay - (time.time() - last_accessed)\n",
    "            if sleep_secs > 0:\n",
    "                # domain has been accessed recently\n",
    "                # so need to sleep\n",
    "                time.sleep(sleep_secs)\n",
    "        # update the last accessed time\n",
    "        self.domains[domain] = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download(url, user_agent='wswp', num_retries=2, charset='utf-8', proxy=None):\n",
    "    \"\"\" Download a given URL and return the page content\n",
    "        args:\n",
    "            url (str): URL\n",
    "        kwargs:\n",
    "            user_agent (str): user agent (default: wswp)\n",
    "            charset (str): charset if website does not include one in headers\n",
    "            proxy (str): proxy url, ex 'http://IP' (default: None)\n",
    "            num_retries (int): number of retries if a 5xx error is seen (default: 2)\n",
    "    \"\"\"\n",
    "    print('Downloading:', url)\n",
    "    request = urllib.request.Request(url)\n",
    "    request.add_header('User-agent', user_agent)\n",
    "    try:\n",
    "        if proxy:\n",
    "            proxy_support = urllib.request.ProxyHandler({'http': proxy})\n",
    "            opener = urllib.request.build_opener(proxy_support)\n",
    "            urllib.request.install_opener(opener)\n",
    "        resp = urllib.request.urlopen(request)\n",
    "        cs = resp.headers.get_content_charset()\n",
    "        if not cs:\n",
    "            cs = charset\n",
    "        html = resp.read().decode(cs)\n",
    "    except (URLError, HTTPError, ContentTooShortError) as e:\n",
    "        print('Download error:', e.reason)\n",
    "        html = None\n",
    "        if num_retries > 0:\n",
    "            if hasattr(e, 'code') and 500 <= e.code < 600:\n",
    "                # recursively retry 5xx HTTP errors\n",
    "                return download(url, num_retries - 1)\n",
    "    return html\n",
    "\n",
    "\n",
    "def get_robots_parser(robots_url):\n",
    "    \" Return the robots parser object using the robots_url \"\n",
    "    rp = robotparser.RobotFileParser()\n",
    "    rp.set_url(robots_url)\n",
    "    rp.read()\n",
    "    return rp\n",
    "\n",
    "\n",
    "def get_links(html):\n",
    "    \" Return a list of links (using simple regex matching) from the html content \"\n",
    "    # a regular expression to extract all links from the webpage\n",
    "    webpage_regex = re.compile(\"\"\"<a[^>]+href=[\"'](.*?)[\"']\"\"\", re.IGNORECASE)\n",
    "    # list of all links from the webpage\n",
    "    return webpage_regex.findall(html)\n",
    "\n",
    "\n",
    "def link_crawler(start_url, link_regex, robots_url=None, user_agent='wswp',\n",
    "                 proxy=None, delay=3, max_depth=4):\n",
    "    \"\"\" Crawl from the given start URL following links matched by link_regex. In the current\n",
    "        implementation, we do not actually scrapy any information.\n",
    "        args:\n",
    "            start_url (str): web site to start crawl\n",
    "            link_regex (str): regex to match for links\n",
    "        kwargs:\n",
    "            robots_url (str): url of the site's robots.txt (default: start_url + /robots.txt)\n",
    "            user_agent (str): user agent (default: wswp)\n",
    "            proxy (str): proxy url, ex 'http://IP' (default: None)\n",
    "            delay (int): seconds to throttle between requests to one domain (default: 3)\n",
    "            max_depth (int): maximum crawl depth (to avoid traps) (default: 4)\n",
    "    \"\"\"\n",
    "    crawl_queue = [start_url]\n",
    "    # keep track which URL's have seen before\n",
    "    seen = {}\n",
    "    if not robots_url:\n",
    "        robots_url = '{}/robots.txt'.format(start_url)\n",
    "    rp = get_robots_parser(robots_url)\n",
    "    throttle = Throttle(delay)\n",
    "    while crawl_queue:\n",
    "        url = crawl_queue.pop()\n",
    "        # check url passes robots.txt restrictions\n",
    "        if rp.can_fetch(user_agent, url):\n",
    "            depth = seen.get(url, 0)\n",
    "            if depth == max_depth:\n",
    "                print('Skipping %s due to depth' % url)\n",
    "                continue\n",
    "            throttle.wait(url)\n",
    "            html = download(url, user_agent=user_agent, proxy=proxy)\n",
    "            if not html:\n",
    "                continue\n",
    "            # TODO: add actual data scraping here\n",
    "            # filter for links matching our regular expression\n",
    "            for link in get_links(html):\n",
    "                if re.match(link_regex, link):\n",
    "                    abs_link = urljoin(start_url, link)\n",
    "                    if abs_link not in seen:\n",
    "                        seen[abs_link] = depth + 1\n",
    "                        crawl_queue.append(abs_link)\n",
    "        else:\n",
    "            print('Blocked by robots.txt:', url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the final version of an advanced web crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
