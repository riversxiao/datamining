{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ending of an URL usually contains a slug and and ID, usually, the slug will be ignored by website, the only thing matters is the ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "why not use it to exploit the entire webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download(url,user_agent='naruto',number_retries = 2, charset='utf-8'):\n",
    "    print(\"Downloading:\", url)\n",
    "    request = urllib.request.Request(url)\n",
    "    request.add_header(\"User-agent\", user_agent)\n",
    "    try:\n",
    "        resp = urllib.request.urlopen(request)\n",
    "        #verify if web charset is utf-8, if not ,we'll use utf-8 to decode, haha, \n",
    "        #it will throw error if no cs returned, but\n",
    "        #we hope utf-8 will help\n",
    "        #read more on pypi.python.org/pypi/chardet to implement a more robust decoder\n",
    "        cs = resp.headers.get_content_charset()\n",
    "        if not cs:\n",
    "            cs = charset\n",
    "        html = resp.read().decode(cs)\n",
    "    except (URLError, HTTPError, ContentTooShortError) as e:\n",
    "        print('Download error:', e.reason)\n",
    "        html = None\n",
    "        if number_retries > 0:\n",
    "            if hasattr(e, \"code\") and 500 <= e.code <= 600:\n",
    "                #recursive download process\n",
    "                return download(url,number_retries -1)\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def crawl_site(url):\n",
    "    for page in itertools.count(1):\n",
    "        pg_url = '{}{}'.format(url,page)\n",
    "        html = download(pg_url)\n",
    "        if html is None:\n",
    "            break\n",
    "        # success - can scrape the result\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one weakness of the above code is that some records may have been deleted, leaving gaps in the database IDs. Then when one of thesee gaps is reached, the crawler will immediately exit. \n",
    "\n",
    "let's see an improved version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crawl_site(url, max_errors = 5):\n",
    "    for page in itertools.count(1):\n",
    "        pg_url = '{}{}'.format(url,page)\n",
    "        html = download(pg_url)\n",
    "        \n",
    "        if html is None:\n",
    "            num_errors += 1\n",
    "            if num_errors == max_error:\n",
    "                break\n",
    "        else:\n",
    "            num_errors =0\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "anyway, a lot of website don't use this way to download the data, for example amazon use ISBN as an ID, which is not pratical if we use the above method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next, I'm introducing the link crawlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
